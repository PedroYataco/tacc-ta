{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import chardet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_language(texto):\n",
    "    replacements = {\n",
    "        # Actualización del idioma\n",
    "        # Actualiza de la letra 'c' a la 'k' si no está seguida de una 'h'\n",
    "        r'c(?![h])': 'k',\n",
    "        # Actualiza de la letra 'qu' a la 'k'\n",
    "        r'qu': 'k',\n",
    "        r'Qu': 'K',\n",
    "        # Actualiza de la palabra 's̈h' por 'x' o\n",
    "        r's̈h': 'x',\n",
    "        r'S̈H': 'X',\n",
    "        # Actualiza de la palabra 'hu[a|e|i]' por 'w[a|e|i]'\n",
    "        r'hu(?=[e|a|i|é|á|í])': 'w',\n",
    "        r'Hu(?=[e|a|i|é|á|í])': 'W',\n",
    "        r'ë': 'e',\n",
    "        r'Ë': 'E',\n",
    "        r'ʆ': 'x',\n",
    "        # r'ã': 'á',\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        texto = re.sub(pattern, replacement, texto)\n",
    "\n",
    "    return texto\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('CSV/U20114818_corpus.csv', sep='|', encoding='Windows-1252')\n",
    "df2.to_csv('CSV/U20114818_corpus_utf-8.csv', index=False, encoding='utf-8', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('CSV/U20172563_corpus.csv', sep=',')\n",
    "df2 = pd.read_csv('CSV/U20114818_corpus_utf-8.csv', sep=',')\n",
    "df3 = pd.read_csv('CSV/U20190876_corpus.csv', sep=';')\n",
    "df4 = pd.read_csv('CSV/U20195539_corpus_utf-8.csv', sep=',')\n",
    "df5 = pd.read_csv('CSV/U20190390_corpus.csv', sep=';')\n",
    "\n",
    "# Unir los dataframes\n",
    "df = pd.concat([df1, df2, df3, df4, df5], ignore_index=True)\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(df, column):\n",
    "    # El body no debe tener cambios de línea\n",
    "    df[column] = df[column].str.replace('\\n', ' ')\n",
    "\n",
    "    # Aplicar update_language a la columna Body\n",
    "    df[column] = df[column].apply(update_language)\n",
    "\n",
    "    # Todo el texto en minúsculas\n",
    "    df[column] = df[column].str.lower()\n",
    "\n",
    "    # Eliminar guiones\n",
    "    df[column] = df[column].str.replace('-', '')\n",
    "    df[column] = df[column].str.replace('—', '')\n",
    "\n",
    "    # Eliminar espacios al inicio y al final\n",
    "    df[column] = df[column].str.strip()\n",
    "\n",
    "    # Todas las vocales sin tilde\n",
    "\n",
    "    # Diccionarios para el mapeo de caracteres\n",
    "    map_upper = {'Á': 'A', 'É': 'E', 'Í': 'I', 'Ó': 'O', 'Ú': 'U'}\n",
    "    map_lower = {'á': 'a', 'é': 'e', 'í': 'i', 'ó': 'o', 'ú': 'u'}\n",
    "\n",
    "    # Aplicar los mapeos\n",
    "    for original, replacement in map_upper.items():\n",
    "        df[column] = df[column].str.replace(original, replacement)\n",
    "\n",
    "    for original, replacement in map_lower.items():\n",
    "        df[column] = df[column].str.replace(original, replacement)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = preprocess_text(df, 'Body')\n",
    "df = preprocess_text(df, 'Title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un vocabulario con las palabras de la columna Body\n",
    "vocabulary = {}\n",
    "\n",
    "def add_words_to_vocabulary(dataframe):\n",
    "    for i, row in dataframe.iterrows():\n",
    "        body = row['Body']\n",
    "        # Eliminar las comillas triples al inicio y al final del cuerpo\n",
    "        body = body[1:-1]\n",
    "        # Separar el cuerpo en palabras\n",
    "        palabras = set(body.split())  # use a set to remove duplicates\n",
    "        for palabra in palabras:\n",
    "            # Eliminar los signos de puntuación\n",
    "            palabra = re.sub(r'[^\\w\\s]', '', palabra)\n",
    "            # Convertir la palabra a minúsculas\n",
    "            palabra = palabra.lower()\n",
    "            if palabra in vocabulary:\n",
    "                vocabulary[palabra] += 1\n",
    "            else:\n",
    "                vocabulary[palabra] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_words_to_vocabulary(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7778"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ordenar el diccionario por frecuencia de palabras\n",
    "sorted_vocabulary = dict(sorted(vocabulary.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_vocabulary.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5695"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener las palabras con solo una ocurrencia\n",
    "palabras_unicas = {k: v for k, v in sorted_vocabulary.items() if v == 1}\n",
    "palabras_unicas.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar en que cuentos se encuentran las palabras únicas\n",
    "def find_unique_words(dataframe):\n",
    "    unique_words = []\n",
    "    for i, row in dataframe.iterrows():\n",
    "        body = row['Body']\n",
    "        # Eliminar las comillas triples al inicio y al final del cuerpo\n",
    "        body = body[1:-1]\n",
    "        # Separar el cuerpo en palabras\n",
    "        palabras = set(body.split())  # use a set to remove duplicates\n",
    "        for palabra in palabras:\n",
    "            # Eliminar los signos de puntuación\n",
    "            palabra = re.sub(r'[^\\w\\s]', '', palabra)\n",
    "            # Convertir la palabra a minúsculas\n",
    "            palabra = palabra.lower()\n",
    "            if palabra in palabras_unicas:\n",
    "                unique_words.append((row['Year'], row['Title'], palabra))\n",
    "    return unique_words\n",
    "\n",
    "unique_words = find_unique_words(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reeplace fake words\n",
    "\n",
    "def replace_fake_words(texto):\n",
    "    replacements = {\n",
    "        r'aqkin': 'akin',\n",
    "        r'kochi': 'koshi',\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        texto = re.sub(pattern, replacement, texto)\n",
    "\n",
    "    return texto\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5695\n",
      "5693\n"
     ]
    }
   ],
   "source": [
    "# mostrar tamaño de unique_words antes de eliminar fake words\n",
    "print(len(unique_words))\n",
    "# Aplicar replace_fake_words a la columna Body\n",
    "df['Body'] = df['Body'].apply(replace_fake_words)\n",
    "unique_words = find_unique_words(df)\n",
    "print(len(unique_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "# Este es el alfabeto shipibo-konibo: < a, b, ch, e, i, j, k, m, n, o, p, r, s, sh, x, t, ts, w, y>\n",
    "# Las vocales son a, e, i, o\n",
    "# Las vocales pueden llevar tilde: á, é, í, ó\n",
    "# Identificar palabras que no pertenecen al alfabeto shipibo-konibo\n",
    "def find_non_shipibo_words(dataframe):\n",
    "    non_shipibo_words = []\n",
    "    for i, row in dataframe.iterrows():\n",
    "        body = row['Body']\n",
    "        body = body[1:-1]\n",
    "        palabras = set(body.split())\n",
    "        for palabra in palabras:\n",
    "            palabra = re.sub(r'[^\\w\\s]', '', palabra)\n",
    "            # palabra = palabra.lower()\n",
    "            if not re.match(r'^[a-záéíó]*(b|ch|e|i|j|k|m|n|o|p|r|s|sh|x|t|ts|w|y|á|é|í|ó)*$', palabra):\n",
    "                non_shipibo_words.append((row['Year'], row['Title'], palabra))\n",
    "    return non_shipibo_words\n",
    "\n",
    "non_shipibo_words = find_non_shipibo_words(df)\n",
    "print(len(non_shipibo_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1979, 'ino betan yawish ini', 'señor'),\n",
       " (1973, 'cabayo betan shinobo ini', 'kabeyo_en'),\n",
       " (2005, 'jaen noa ika mayai', '5'),\n",
       " (2005, 'jaen noa ika mayai', '2816'),\n",
       " (2005, 'jaen noa ika mayai', '9'),\n",
       " (2005, 'jaen noa ika mayai', '194'),\n",
       " (2005, 'jaen noa ika mayai', '506'),\n",
       " (2005, 'jaen noa ika mayai', '154'),\n",
       " (2005, 'jaen noa ika mayai', '247'),\n",
       " (2005, 'jaen noa ika mayai', '1993'),\n",
       " (2005, 'jaskatash limonjema pikota', '32'),\n",
       " (2019, 'maxo naikan kani', 'cañiwa'),\n",
       " (2019, 'maxo naikan kani', 'kañiwa'),\n",
       " (2015, 'anishaman joa ikaxbi basimabires otsiax mawatai', '40'),\n",
       " (2015, 'anishaman joa ikaxbi basimabires otsiax mawatai', '70'),\n",
       " (2015, 'anishaman joa ikaxbi basimabires otsiax mawatai', '3'),\n",
       " (2023, 'kena jaweki irake aka i', 'b5'),\n",
       " (2023, 'kena jaweki irake aka i', 'a17'),\n",
       " (2023, 'kena jaweki irake aka i', '6maotianra'),\n",
       " (2023, 'kena jaweki irake aka i', 'a17'),\n",
       " (2023, 'kena jaweki irake aka i', '1'),\n",
       " (2023, 'jain pacho maw kanai karretera', '6'),\n",
       " (2023, 'peokin yoia', '1'),\n",
       " (2023, 'kecwa jonibo onanwe', 'cañaris'),\n",
       " (2023, 'awajun jonibo onanwe', '27'),\n",
       " (2023, 'chomo ati onanwe', '44'),\n",
       " (2023, 'yine joniboan kene', '48'),\n",
       " (2023, 'yoinabo namiati', '90'),\n",
       " (2023, 'aimara jonibo onanwe', '2010'),\n",
       " (2023, 'wampis jonibo onanwe', '2006'),\n",
       " (2023, 'wampis jonibo onanwe', '2000'),\n",
       " (2023, 'wampis jonibo onanwe', '2008'),\n",
       " (2022, 'pies negros jonibo onanwe', '1730'),\n",
       " (2022, 'pies negros jonibo onanwe', '1900'),\n",
       " (2022, 'pies negros jonibo onanwe', '25000'),\n",
       " (2022, 'pies negros jonibo onanwe', '6000'),\n",
       " (2023, 'pitumarka ikainxon yoman timakanaibo', 'vikuña'),\n",
       " (2021, 'ewe yura e weruwi', 'puña'),\n",
       " (2021, 'yapawakeshtaju', 'wakameraiñama')]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_shipibo_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar las letras que no pertenecen al alfabeto shipibo-konibo a excepción de números y la letra 'ñ'\n",
    "def remove_non_shipibo_letters(texto):\n",
    "    replacements = {\n",
    "        r'[^a-zA-Záéíóñ0-9,\\.;\\-\\'\\s]': '',\n",
    "    }\n",
    "\n",
    "    for pattern, replacement in replacements.items():\n",
    "        texto = re.sub(pattern, replacement, texto)\n",
    "\n",
    "    return texto\n",
    "\n",
    "\n",
    "# non_shipibo_words_before = find_non_shipibo_words(df)\n",
    "# print(len(non_shipibo_words))\n",
    "# Aplicar remove_non_shipibo_letters a la columna Body\n",
    "df['Body'] = df['Body'].apply(remove_non_shipibo_letters)\n",
    "#  Eliminar espacios extra dentro de los strings\n",
    "df['Body'] = df['Body'].str.replace(' +', ' ', regex=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el dataframe en un archivo CSV con las columnas Year, Title, Body y Category\n",
    "df.to_csv('CSV/corpus.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non_shipibo_words = find_non_shipibo_words(df)\n",
    "# print(len(non_shipibo_words))\n",
    "# non_shipibo_words\n",
    "\n",
    "# # words which have been removed\n",
    "# removed_words = list(set(non_shipibo_words_before) - set(non_shipibo_words))\n",
    "# removed_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el diccionario en un archivo CSV\n",
    "# with open('CSV/vocabulary.csv', 'w', encoding='utf-8') as archivo:\n",
    "#     archivo.write('Word,Frequency\\n')\n",
    "#     for palabra, frecuencia in sorted_vocabulary.items():\n",
    "#         archivo.write(f'{palabra},{frecuencia}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
